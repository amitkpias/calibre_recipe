from __future__ import with_statement
__license__ = 'GPL 3'
__copyright__ = '2014, Amit <amitkp.ias@gmail.com>'

from calibre.web.feeds.news import BasicNewsRecipe
import re, time, urllib2
from functools import wraps

class My_Feeds(BasicNewsRecipe):
    title                  = 'GKToday'
    language               = 'en_IN'
    oldest_article         = 20
    __author__             = 'Amit'
    max_articles_per_feed  = 10000
    timeout                = 600
    no_stylesheets         = True
    remove_javascript      = True
    reverse_article_order  = True
    masthead_url           = 'http://currentaffairs.gktoday.in/wp-content/themes/Infinity/images/logo.jpg'
    publisher              = 'gktoday.in'
    
    keep_only_tags = [dict(id=['content'])]
    remove_tags = [dict(attrs={'class':['adsbygoogle','chota','meta_comments']}), dict(id=['respond','section','comments'])]
    
    conversion_options = { 'tags': 'GKToday,Current-Affairs,News'}
    extra_css = ''' .meta_tags{font-size: 60%;}
                    .title{font-size: 150%;}
                    .meta_date{font-size: 60%;}
                    body{font-family: Arial,Helvetica,sans-serif}
                '''
    def retry(ExceptionToCheck, tries=10, delay=3, backoff=2, logger=None):
        """Retry calling the decorated function using an exponential backoff.

        http://www.saltycrane.com/blog/2009/11/trying-out-retry-decorator-python/
        original from: http://wiki.python.org/moin/PythonDecoratorLibrary#Retry

        :param ExceptionToCheck: the exception to check. may be a tuple of
            exceptions to check
        :type ExceptionToCheck: Exception or tuple
        :param tries: number of times to try (not retry) before giving up
        :type tries: int
        :param delay: initial delay between retries in seconds
        :type delay: int
        :param backoff: backoff multiplier e.g. value of 2 will double the delay
            each retry
        :type backoff: int
        :param logger: logger to use. If None, print
        :type logger: logging.Logger instance
        """
        def deco_retry(f):

            @wraps(f)
            def f_retry(*args, **kwargs):
                mtries, mdelay = tries, delay
                while mtries > 1:
                    try:
                        return f(*args, **kwargs)
                    except ExceptionToCheck, e:
                        msg = "%s, Retrying in %d seconds..." % (str(e), mdelay)
                        if logger:
                            logger.warning(msg)
                        else:
                            print msg
                        time.sleep(mdelay)
                        mtries -= 1
                        mdelay *= backoff
                return f(*args, **kwargs)

            return f_retry  # true decorator

        return deco_retry    
    @retry(urllib2.URLError, tries=4, delay=3, backoff=2)
    def url2soup(self,url):
        #response = urllib2.urlopen('http://python.org/')
        #html = response.read()        
        return self.index_to_soup(url)
        
    def slog(self,text):
        print "----> " + text
        pass
    
    def parse_index(self):
        months = [  #['Jan 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-january-2014'],
                    #['Feb 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-february-2014'],
                    #['March 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-march-2014'],
                    #['April 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-april-2014'],
                    #['May 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-may-2014'],
                    #['June 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-june-2014'],
                    ['July 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-july-2014'],
                    ['August 2014','http://currentaffairs.gktoday.in/category/current-affairs-2014/current-affairs-august-2014']
                ]
        feeds = []
        for month in months:
            current_articles = []
            current_section = month[0]
            soup = self.url2soup(month[1])
            x = soup.find("a",{"class":"last",})
            m = re.findall(r"\/(\d*)$", x["href"])
            if m:
                last = m[0] 
            else:
                self.abort_recipe_processing("stopping parse_index : Could not find the last page")
            for page in range(1,int(last)+1):
                if page != 1:
                    page_url = month[1] + '/page/' + str(page)
                else:
                    page_url = month[1]
                self.slog("parsing page :" + page_url)
                page_soup = self.url2soup(page_url)
                div = page_soup.find(id='content')
                for a in div.findAll("a",{"rel":"bookmark",}):
                    current_articles.append({'url':a['href'], 'title':self.tag_to_string(a), 'date': '', 'description':''})
                    self.slog("Added article " + a['href'])
            feeds.append((current_section, current_articles))
            self.slog('Added Section: ' + current_section )
        return feeds
